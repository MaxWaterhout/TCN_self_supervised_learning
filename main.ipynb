{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TCNmodel\n",
    "import tfrecord\n",
    "import torch\n",
    "import numpy as np\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "\n",
    "# # load video\n",
    "# PAD_WIDTH = 256\n",
    "# def pad_sequence_feats(data):\n",
    "#     context, features = data\n",
    "#     for k, v in features.items():\n",
    "#         features[k] = np.pad(v, ((0, PAD_WIDTH - len(v)), (0, 0)), 'constant')\n",
    "#     return (context, features)\n",
    "\n",
    "# context_description = {\"len\": \"int\", \"task\": \"byte\"}\n",
    "# sequence_description = {\"view0\":\"byte\" , \"view1\": \"byte\"}\n",
    "# dataset = TFRecordDataset(\"../pouring/multiview-pouring/tfrecords/train/whitewater_to_clear2_real.tfrecord\",\n",
    "#                           index_path=None,\n",
    "#                           description=context_description,\n",
    "#                           #transform=pad_sequence_feats,\n",
    "#                           sequence_description=sequence_description)\n",
    "# loader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "# data = next(iter(loader))\n",
    "# print(data[0]['len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256]\n",
      "[116 114  97 105 110  47 119 104 105 116 101 119  97 116 101 114  95 116\n",
      " 111  95  99 108 101  97 114  50  95 114 101  97 108]\n",
      "(28742,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9580.666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_description = {\"len\": \"int\", \"task\": \"byte\"}\n",
    "sequence_description = {\"view0\":\"byte\" , \"view1\": \"byte\"}\n",
    "loader = tfrecord.tfrecord_loader(\"../pouring/multiview-pouring/tfrecords/train/whitewater_to_clear2_real.tfrecord\", None,\n",
    "                                  context_description,\n",
    "                                  sequence_description=sequence_description)\n",
    "\n",
    "for context, sequence_feats in loader:\n",
    "    print(context[\"len\"])\n",
    "    #print(sequence_feats[\"view0\"])\n",
    "    break\n",
    "context['len']\n",
    "print(context['task'])\n",
    "print(sequence_feats['view1'][255].shape)\n",
    "28742/3\n",
    "# context feat dictionary [len task]\n",
    "# sequence)feat dictionary [view0 view1]\n",
    "# total of 256 frames, each frame has different pixel numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Not compiled with video_reader support, to enable video_reader support, please install ffmpeg (version 4.2 is currently supported) and build torchvision from source.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d02f691168dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"video\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\io\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, stream, num_threads, device)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_has_video_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m    131\u001b[0m                 \u001b[1;34m\"Not compiled with video_reader support, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[1;34m\"to enable video_reader support, please install \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Not compiled with video_reader support, to enable video_reader support, please install ffmpeg (version 4.2 is currently supported) and build torchvision from source."
     ]
    }
   ],
   "source": [
    "# # load one video\n",
    "# import torchvision\n",
    "# from torchvision.datasets.utils import download_url\n",
    "# video_path = '../pouring/multiview-pouring/videos/train/whitewater_to_clear2_real_view1.mov'\n",
    "# print(torch.__version__)\n",
    "# stream = \"video\"\n",
    "# video = torchvision.io.VideoReader(video_path, stream)\n",
    "# video.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9b5ac4770ed6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mIMAGE_SIZE_H\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_SIZE_W\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_parse_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# @tf.function\n",
    "# def resize(img):\n",
    "#     return tf.image.resize(img, [IMAGE_SIZE_H, IMAGE_SIZE_W])\n",
    "\n",
    "# def _parse_example(example_proto):\n",
    "#   # Parse the input tf.train.Example using the dictionary above.\n",
    "#     context, sequence = tf.parse_single_sequence_example(example_proto,\n",
    "#                                                        context_features=context_features, \n",
    "#                                                        sequence_features=sequence_features)\n",
    "#   # extract the expected shape \n",
    "#     shape = (context['temporal'], context['height'], context['width'], context['depth'])\n",
    "  \n",
    "#   ## the golden while loop ## \n",
    "#   # loop through the feature lists and decode each image seperately \n",
    "  \n",
    "#   # decoding the first video \n",
    "#     video_data = tf.image.decode_image(tf.gather(sequence['video_frames'], [0])[0])\n",
    "#     video_data = tf.expand_dims(video_data, 0)\n",
    "  \n",
    "#     i = tf.constant(1, dtype=tf.int32)\n",
    "#   # condition of when to stop / loop through every frame\n",
    "#     cond = lambda i, _: tf.less(i, tf.cast(context['temporal'], tf.int32))\n",
    "  \n",
    "#   # reading + decoding the i-th image frame \n",
    "#     def body(i, video_data):\n",
    "#       # get the i-th index \n",
    "#         encoded_img = tf.gather(sequence['video_frames'], [i])\n",
    "#       # decode the image \n",
    "#         img_data = tf.image.decode_image(encoded_img[0]) \n",
    "#       # append to list using tf operations \n",
    "#         video_data = tf.concat([video_data, [img_data]], 0)\n",
    "#       # update counter & new video_data \n",
    "#         return (tf.add(i, 1), video_data)\n",
    "\n",
    "#     _, video_data = tf.while_loop(cond, body, [i, video_data], \n",
    "#                                   shape_invariants=[i.get_shape(), tf.TensorShape([None])])\n",
    "#   # use this to set the shape + dtype\n",
    "#     video_data = tf.reshape(video_data, shape)\n",
    "#     video_data = tf.cast(video_data, tf.float32)\n",
    "  \n",
    "#   # resize each frame in video -- can apply different augmentations etc. like this \n",
    "#     video_data = tf.map_fn(resize, video_data, back_prop=False, parallel_iterations=10)\n",
    "  \n",
    "#     label = context['label']\n",
    "#   # return the data example and its corresponding label \n",
    "#     return video_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4619dbeacbf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.tfrecord'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_parse_image_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnext_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset('train.tfrecord')\\\n",
    "        .map(_parse_image_function)\\\n",
    "        .batch(2)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# use standard tf training setup \n",
    "with tf.Session() as sess: \n",
    "    batch_vid, batch_label = sess.run(next_element)\n",
    "    print(batch_vid.shape, batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
